#!/usr/bin/env python3
"""
Analyseur de contexte dynamique pour Isskar
Extrait automatiquement les entit√©s depuis les documents Confluence
"""

import re
import logging
from typing import List, Dict, Set
from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import defaultdict

from .isskar_context_analyzer import QueryType, IsskarContext  # Garde les types


@dataclass
class EntityInfo:
    """Information sur une entit√© d√©tect√©e"""

    name: str
    type: str  # 'project', 'person', 'technology', etc.
    frequency: int
    last_seen: datetime
    contexts: Set[str]  # Contextes o√π elle appara√Æt
    confidence: float


class DynamicContextAnalyzer:
    """
    Analyseur de contexte qui apprend dynamiquement depuis les documents Confluence
    Pas de donn√©es cod√©es en dur - tout est inf√©r√© des documents existants
    """

    def __init__(self, vector_db=None):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.vector_db = vector_db
        
        self.logger.info("üöÄ Initialisation DynamicContextAnalyzer")
        if vector_db:
            self.logger.info("üìä Vector DB disponible pour l'apprentissage")
        else:
            self.logger.warning("‚ö†Ô∏è Pas de Vector DB - fonctionnement limit√©")

        # Cache des entit√©s apprises (expiration 1 heure)
        self._entity_cache = {}
        self._cache_timestamp = None
        self._cache_duration = timedelta(hours=1)

        # Patterns g√©n√©riques pour la d√©tection d'entit√©s
        self._init_generic_patterns()
        self.logger.info("‚úÖ DynamicContextAnalyzer initialis√©")

    def _init_generic_patterns(self):
        """Initialise les patterns g√©n√©riques de d√©tection"""

        # Patterns pour d√©tecter les noms de personnes dans les documents
        self.person_patterns = [
            r"par\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+\([^)]*ISSKAR\)",  # "par Johan JUBLANC (ISSKAR)"
            r'Document:\s+"[^"]*"\s+par\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',  # Dans les m√©tadonn√©es
            r"auteur[:\s]+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",  # "auteur: Johan JUBLANC"
            r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:travaille|s\'occupe|responsable)",  # "Johan travaille sur"
        ]

        # Patterns pour d√©tecter les projets
        self.project_patterns = [
            r"projet\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",  # "projet Teora"
            r"sur\s+le\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",  # "sur le Teora"
            r'Document:\s+"([A-Z][a-z]+(?:\s+[A-Z-]+)*)[^"]*"',  # Titre de document
            r"Mission[^>]*>\s*([A-Z][a-z]+(?:\s+[A-Z-]+)*)",  # Hi√©rarchie Confluence
        ]

        # Patterns pour d√©tecter les technologies
        self.technology_patterns = [
            r"\b(Vue|React|Angular|Python|Java|Docker|Kubernetes|k8s)\b",
            r"\b(API|REST|GraphQL|JWT|OAuth)\b",
            r"\b(MySQL|PostgreSQL|MongoDB|Redis)\b",
            r"\b(AWS|Azure|GCP|Weaviate|Elasticsearch)\b",
        ]

        # Patterns contextuels pour am√©liorer la d√©tection
        self.contextual_indicators = {
            "pronouns": ["leurs", "ses", "ces", "celui", "ceux", "leur", "sa", "son", "ce", "cette"],
            "references": ["cette mission", "ce projet", "cette √©quipe"],
            "relationship_words": ["responsabilit√©s", "r√¥les", "contact", "√©quipe", "travaille"],
        }

    def _is_cache_valid(self) -> bool:
        """V√©rifie si le cache est encore valide"""
        if not self._cache_timestamp:
            return False
        return datetime.now() - self._cache_timestamp < self._cache_duration

    def _learn_from_documents(self) -> Dict[str, EntityInfo]:
        """Apprend les entit√©s depuis les documents Confluence"""

        if not self.vector_db:
            self.logger.warning("Pas de vector_db disponible, utilisation du cache existant")
            return self._entity_cache.get("entities", {})

        self.logger.info("üîç Apprentissage des entit√©s depuis Confluence...")

        entities = defaultdict(lambda: {"frequency": 0, "contexts": set(), "last_seen": datetime.now()})

        try:
            # R√©cup√®re un √©chantillon de documents r√©cents
            self.logger.info("üìÑ R√©cup√©ration de documents r√©cents pour apprentissage...")
            documents = self._get_recent_documents(limit=200)
            self.logger.info(f"üìÑ {len(documents)} documents r√©cup√©r√©s")

            total_persons = set()
            total_projects = set()
            total_technologies = set()
            
            for i, doc in enumerate(documents):
                content = doc.get("content", "")
                metadata = doc.get("metadata", {})

                # Extraction des personnes
                persons = self._extract_persons(content, metadata)
                total_persons.update(persons)
                for person in persons:
                    key = f"person:{person.lower()}"
                    entities[key]["frequency"] += 1
                    entities[key]["contexts"].add("team_management")
                    entities[key]["type"] = "person"
                    entities[key]["name"] = person

                # Extraction des projets
                projects = self._extract_projects(content, metadata)
                total_projects.update(projects)
                for project in projects:
                    key = f"project:{project.lower()}"
                    entities[key]["frequency"] += 1
                    entities[key]["contexts"].add("project_management")
                    entities[key]["type"] = "project"
                    entities[key]["name"] = project

                # Extraction des technologies
                technologies = self._extract_technologies(content)
                total_technologies.update(technologies)
                for tech in technologies:
                    key = f"technology:{tech.lower()}"
                    entities[key]["frequency"] += 1
                    entities[key]["contexts"].add("technical")
                    entities[key]["type"] = "technology"
                    entities[key]["name"] = tech
                    
                if (i + 1) % 50 == 0:
                    self.logger.info(f"üìÑ Trait√© {i + 1}/{len(documents)} documents...")
                    
            self.logger.info(f"üè∑Ô∏è Entit√©s extraites - Personnes: {len(total_persons)}, Projets: {len(total_projects)}, Technologies: {len(total_technologies)}")
            if total_persons:
                self.logger.info(f"üë• Personnes trouv√©es: {list(total_persons)[:5]}{'...' if len(total_persons) > 5 else ''}")
            if total_projects:
                self.logger.info(f"üìã Projets trouv√©s: {list(total_projects)[:5]}{'...' if len(total_projects) > 5 else ''}")
            if total_technologies:
                self.logger.info(f"‚öôÔ∏è Technologies trouv√©es: {list(total_technologies)[:5]}{'...' if len(total_technologies) > 5 else ''}")

            # Conversion en EntityInfo avec calcul de confiance
            learned_entities = {}
            for key, data in entities.items():
                confidence = min(1.0, data["frequency"] / 10.0)  # Confiance bas√©e sur la fr√©quence
                if confidence > 0.1:  # Seuil minimum
                    learned_entities[key] = EntityInfo(
                        name=data["name"],
                        type=data["type"],
                        frequency=data["frequency"],
                        last_seen=data["last_seen"],
                        contexts=data["contexts"],
                        confidence=confidence,
                    )

            high_confidence = sum(1 for e in learned_entities.values() if e.confidence > 0.7)
            self.logger.info(f"‚úÖ {len(learned_entities)} entit√©s apprises depuis Confluence ({high_confidence} haute confiance)")
            
            # Log des entit√©s les plus fr√©quentes par type
            by_type = {"person": [], "project": [], "technology": []}
            for entity in learned_entities.values():
                if entity.type in by_type:
                    by_type[entity.type].append((entity.name, entity.frequency, entity.confidence))
            
            for entity_type, entities_list in by_type.items():
                if entities_list:
                    sorted_entities = sorted(entities_list, key=lambda x: x[1], reverse=True)[:3]
                    entities_str = ", ".join([f"{name}({freq}, {conf:.2f})" for name, freq, conf in sorted_entities])
                    self.logger.info(f"üîù Top {entity_type}s: {entities_str}")
            
            return learned_entities

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'apprentissage: {e}")
            return {}

    def _get_recent_documents(self, limit: int = 200) -> List[Dict]:
        """R√©cup√®re des documents r√©cents depuis la base vectorielle"""

        try:
            # Utilise une requ√™te g√©n√©rique pour r√©cup√©rer des documents
            sample_queries = [
                "ISSKAR projet √©quipe",
                "document confluence",
                "r√©union meeting",
                "d√©veloppement technical",
            ]

            all_docs = []
            for query in sample_queries:
                if len(all_docs) >= limit:
                    break

                try:
                    results = self.vector_db.search(query, k=min(50, limit - len(all_docs)))
                    for result in results:
                        doc_data = {"content": result.document.content, "metadata": result.document.metadata or {}}
                        all_docs.append(doc_data)
                except Exception as e:
                    self.logger.debug(f"Erreur requ√™te '{query}': {e}")
                    continue

            self.logger.info(f"üìÑ {len(all_docs)} documents r√©cup√©r√©s pour apprentissage")
            return all_docs

        except Exception as e:
            self.logger.error(f"Erreur r√©cup√©ration documents: {e}")
            return []

    def _extract_persons(self, content: str, metadata: Dict) -> Set[str]:
        """Extrait les noms de personnes du contenu"""
        persons = set()

        # Extraction depuis les m√©tadonn√©es d'auteur
        author = metadata.get("author_name", "")
        if author and len(author.split()) <= 3:  # √âvite les phrases compl√®tes
            persons.add(author.strip())

        # Extraction avec patterns
        for pattern in self.person_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                name = match.strip()
                if len(name.split()) <= 3 and name.replace(" ", "").isalpha():
                    persons.add(name)

        return persons

    def _extract_projects(self, content: str, metadata: Dict) -> Set[str]:
        """Extrait les noms de projets du contenu"""
        projects = set()

        # Extraction depuis le titre et hi√©rarchie
        title = metadata.get("title", "")
        hierarchy = metadata.get("hierarchy_breadcrumb", "")

        for text in [title, hierarchy]:
            for pattern in self.project_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0]
                    project = match.strip()
                    if 2 <= len(project) <= 20 and project.lower() not in ["document", "r√©union", "meeting"]:
                        projects.add(project)

        # Extraction depuis le contenu
        for pattern in self.project_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                project = match.strip()
                if 2 <= len(project) <= 20:
                    projects.add(project)

        return projects

    def _extract_technologies(self, content: str) -> Set[str]:
        """Extrait les technologies du contenu"""
        technologies = set()

        for pattern in self.technology_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                technologies.add(match)

        return technologies

    def get_learned_entities(self) -> Dict[str, EntityInfo]:
        """Retourne les entit√©s apprises (avec cache)"""

        if self._is_cache_valid():
            cached_entities = self._entity_cache.get("entities", {})
            self.logger.info(f"üíæ Utilisation du cache ({len(cached_entities)} entit√©s, √¢ge: {datetime.now() - self._cache_timestamp})")
            return cached_entities

        # Renouvelle le cache
        self.logger.info("üîÑ Cache expir√©, renouvellement des entit√©s...")
        entities = self._learn_from_documents()
        self._entity_cache = {"entities": entities}
        self._cache_timestamp = datetime.now()
        self.logger.info(f"‚úÖ Cache renouvel√© avec {len(entities)} entit√©s")

        return entities

    def classify_query(self, original_query: str, enriched_query: str) -> QueryType:
        """
        Classification intelligente bas√©e sur les entit√©s apprises dynamiquement
        """
        self.logger.info(f"ü§ñ Classification query: '{original_query}' (enriched: {len(enriched_query) > len(original_query)})")
        query_lower = original_query.lower().strip()

        # D√©tection salutations (patterns statiques OK)
        greeting_patterns = [
            r"^(bonjour|salut|hello|hi|hey|coucou)[\s!.]*$",
            r"^(merci|thank you|thanks)[\s!.]*$",
            r"^(au revoir|bye|goodbye|√† bient√¥t)[\s!.]*$",
        ]

        if any(re.match(pattern, query_lower) for pattern in greeting_patterns):
            self.logger.info("üëã D√©tect√© comme GREETING")
            return QueryType.GREETING

        # R√©cup√®re les entit√©s apprises
        entities = self.get_learned_entities()

        # D√©tection contextuelles avec r√©f√©rences implicites
        has_pronouns = any(pronoun in query_lower for pronoun in self.contextual_indicators["pronouns"])
        has_references = any(ref in query_lower for ref in self.contextual_indicators["references"])
        context_enriched = len(enriched_query) > len(original_query) * 1.3

        if has_pronouns or has_references or context_enriched:
            self.logger.info(f"üß© D√©tect√© comme CONTEXTUAL (pronouns:{has_pronouns}, refs:{has_references}, enriched:{context_enriched})")
            return QueryType.CONTEXTUAL

        # D√©tection mentions explicites de projets appris
        for key, entity in entities.items():
            if entity.type == "project" and entity.name.lower() in query_lower:
                self.logger.info(f"üìã D√©tect√© comme PROJECT_SPECIFIC (projet: {entity.name})")
                return QueryType.PROJECT_SPECIFIC

        # D√©tection mentions explicites de personnes apprises
        for key, entity in entities.items():
            if entity.type == "person" and any(part.lower() in query_lower for part in entity.name.split()):
                self.logger.info(f"üë• D√©tect√© comme TEAM_INQUIRY (personne: {entity.name})")
                return QueryType.TEAM_INQUIRY

        # D√©tection technique
        for key, entity in entities.items():
            if entity.type == "technology" and entity.name.lower() in query_lower:
                self.logger.info(f"‚öôÔ∏è D√©tect√© comme TECHNICAL (tech: {entity.name})")
                return QueryType.TECHNICAL

        # D√©tection mots business
        business_keywords = ["budget", "co√ªt", "prix", "commercial", "client", "deal"]
        if any(keyword in query_lower for keyword in business_keywords):
            found_keyword = next(keyword for keyword in business_keywords if keyword in query_lower)
            self.logger.info(f"üíº D√©tect√© comme BUSINESS (mot-cl√©: {found_keyword})")
            return QueryType.BUSINESS

        # D√©tection termes g√©n√©riques probl√©matiques
        generic_terms = {"test", "help", "aide", "info", "ok"}
        words = query_lower.split()
        if len(words) == 1 and words[0] in generic_terms:
            self.logger.info(f"‚ö†Ô∏è D√©tect√© comme GENERIC (terme: {words[0]})")
            return QueryType.GENERIC

        # Par d√©faut: projet sp√©cifique (contexte Isskar)
        self.logger.info("üìã D√©tect√© comme PROJECT_SPECIFIC (d√©faut)")
        return QueryType.PROJECT_SPECIFIC

    def extract_isskar_entities(self, query: str, enriched_query: str) -> IsskarContext:
        """
        Extraction d'entit√©s bas√©e on l'apprentissage dynamique
        """
        self.logger.info(f"üè∑Ô∏è Extraction entit√©s depuis: '{enriched_query[:100]}{'...' if len(enriched_query) > 100 else ''}''")
        entities = self.get_learned_entities()

        projects = set()
        team_members = set()
        technical_domains = set()

        # Recherche dans la query enrichie
        text_to_analyze = enriched_query.lower()

        for key, entity in entities.items():
            entity_name_lower = entity.name.lower()

            if entity.type == "project" and entity_name_lower in text_to_analyze:
                projects.add(entity_name_lower)
            elif entity.type == "person":
                # Recherche par pr√©nom ou nom complet
                name_parts = entity.name.lower().split()
                if any(part in text_to_analyze for part in name_parts):
                    team_members.add(name_parts[0])  # Utilise le pr√©nom
            elif entity.type == "technology" and entity_name_lower in text_to_analyze:
                technical_domains.add(entity_name_lower)

        self.logger.info(f"üè∑Ô∏è Entit√©s extraites - Projets: {projects}, Personnes: {team_members}, Technologies: {technical_domains}")
        
        return IsskarContext(
            projects=projects,
            team_members=team_members,
            mission_types=set(),  # √Ä impl√©menter si n√©cessaire
            technical_domains=technical_domains,
            business_areas=set(),  # √Ä impl√©menter si n√©cessaire
            hierarchical_context=[],  # √Ä impl√©menter si n√©cessaire
        )

    def get_filtering_strategy(self, query_type: QueryType, context: IsskarContext) -> Dict:
        """
        Strat√©gie de filtrage adaptative (inchang√©e)
        """
        strategies = {
            QueryType.CONTEXTUAL: {
                "score_threshold": 0.32,
                "relevance_threshold": 0.05,
                "trust_vector_similarity": True,
                "apply_synonym_matching": True,
                "context_boost": 0.15,
            },
            QueryType.PROJECT_SPECIFIC: {
                "score_threshold": 0.38,
                "relevance_threshold": 0.08,
                "trust_vector_similarity": True,
                "apply_synonym_matching": True,
                "context_boost": 0.10,
            },
            QueryType.TEAM_INQUIRY: {
                "score_threshold": 0.35,
                "relevance_threshold": 0.10,
                "trust_vector_similarity": True,
                "apply_synonym_matching": False,
                "context_boost": 0.08,
            },
            QueryType.TECHNICAL: {
                "score_threshold": 0.42,
                "relevance_threshold": 0.15,
                "trust_vector_similarity": False,
                "apply_synonym_matching": True,
                "context_boost": 0.05,
            },
            QueryType.BUSINESS: {
                "score_threshold": 0.40,
                "relevance_threshold": 0.12,
                "trust_vector_similarity": False,
                "apply_synonym_matching": True,
                "context_boost": 0.05,
            },
            QueryType.GENERIC: {
                "score_threshold": 0.70,
                "relevance_threshold": 0.40,
                "trust_vector_similarity": False,
                "apply_synonym_matching": False,
                "context_boost": 0.0,
            },
            QueryType.GREETING: {
                "score_threshold": 1.0,
                "relevance_threshold": 1.0,
                "trust_vector_similarity": False,
                "apply_synonym_matching": False,
                "context_boost": 0.0,
            },
        }

        return strategies.get(query_type, strategies[QueryType.PROJECT_SPECIFIC])

    def get_learned_stats(self) -> Dict:
        """Retourne des statistiques sur l'apprentissage"""
        entities = self.get_learned_entities()

        stats = {
            "total_entities": len(entities),
            "by_type": defaultdict(int),
            "high_confidence": 0,
            "last_learning": self._cache_timestamp.isoformat() if self._cache_timestamp else None,
        }

        for entity in entities.values():
            stats["by_type"][entity.type] += 1
            if entity.confidence > 0.7:
                stats["high_confidence"] += 1

        return dict(stats)
