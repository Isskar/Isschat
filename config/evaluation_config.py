"""
Configuration for the RAG Evaluation System
"""

import os
import importlib.util
from dataclasses import dataclass, field
from typing import Dict, ClassVar, Optional
from functools import lru_cache
from enum import Enum

# Import the main Isschat config directly from the file to avoid circular imports
config_file_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "config.py")
spec = importlib.util.spec_from_file_location("isschat_config", config_file_path)
isschat_config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(isschat_config_module)
get_isschat_config = isschat_config_module.get_config


class DatabaseType(str, Enum):
    """Database types for evaluation storage"""

    SQLITE = "sqlite"
    MOCK = "mock"


class LLMBackend(str, Enum):
    """LLM backend types"""

    PYDANTIC_AI = "pydantic_ai"
    OPENAI = "openai"


@dataclass
class LLMConfig:
    """Configuration for LLM used in evaluation"""

    model_name: str = "openai/gpt-4-turbo"
    api_key: str = ""
    api_base: str = "https://openrouter.ai/api/v1"
    temperature: float = 0.1
    max_tokens: int = 2000
    timeout: int = 30


@dataclass
class DatabaseConfig:
    """Configuration for evaluation database"""

    type: DatabaseType = DatabaseType.SQLITE
    path: str = "data/evaluation/results/evaluation_results.db"
    backup_enabled: bool = True
    backup_interval_hours: int = 24


@dataclass
class PromptConfig:
    """Configuration for evaluation prompts"""

    generation_evaluation_prompt: str = """
You are an expert judge evaluating the quality of responses generated by a RAG (Retrieval-Augmented Generation) system.

Please evaluate the following response on these 5 criteria (score 0-10 for each):

1. **Relevance**: How well does the response address the question asked?
2. **Accuracy**: Are the facts and information provided correct?
3. **Completeness**: Is the response complete and comprehensive?
4. **Clarity**: Is the response clear, well-structured, and easy to understand?
5. **Source Usage**: How well does the response utilize the provided sources?

**Question**: {question}
**Context/Sources**: {context}
**Response**: {response}
{expected_answer_section}

Provide your evaluation in the following JSON format:
{{
    "relevance": <score 0-10>,
    "accuracy": <score 0-10>,
    "completeness": <score 0-10>,
    "clarity": <score 0-10>,
    "source_usage": <score 0-10>,
    "overall_score": <average score>,
    "justification": "Detailed explanation of your scoring"
}}
"""

    robustness_evaluation_prompt: str = """
You are an expert judge evaluating the robustness of a RAG system response.

Test Type: {test_type}
Expected Behavior: {expected_behavior}

**Question**: {question}
**Response**: {response}

Evaluate if the response demonstrates the expected behavior for this robustness test.
Score 0-10 where:
- 0-3: Completely fails the robustness test
- 4-6: Partially meets expectations
- 7-8: Mostly meets expectations  
- 9-10: Perfectly demonstrates expected behavior

Provide your evaluation in JSON format:
{{
    "score": <score 0-10>,
    "passed": <true/false>,
    "justification": "Detailed explanation of why the response passes/fails this robustness test"
}}
"""


@dataclass
class ReportingConfig:
    """Configuration for reporting and output"""

    output_dir: str = "data/evaluation/results"
    generate_pdf: bool = True
    generate_json: bool = True
    generate_csv: bool = True
    dashboard_port: int = 8503


@dataclass
class EvaluationConfig:
    """General configuration for the evaluation system"""

    llm: LLMConfig = field(default_factory=LLMConfig)
    database: DatabaseConfig = field(default_factory=DatabaseConfig)
    prompts: PromptConfig = field(default_factory=PromptConfig)
    reporting: ReportingConfig = field(default_factory=ReportingConfig)

    # Dataset paths
    test_datasets: Dict[str, str] = field(
        default_factory=lambda: {
            "robustness": "data/evaluation/datasets/robustness_tests.tsv",
            "performance": "data/evaluation/datasets/performance_tests.tsv",
            "quality": "data/evaluation/datasets/quality_tests.tsv",
            "consistency": "data/evaluation/datasets/consistency_tests.tsv",
        }
    )

    # Evaluation thresholds
    thresholds: Dict[str, float] = field(
        default_factory=lambda: {
            "min_accuracy": 7.0,
            "min_relevance": 7.0,
            "min_completeness": 6.0,
            "min_clarity": 6.0,
            "min_source_usage": 6.0,
            "max_response_time": 5.0,
            "min_satisfaction_rate": 0.75,
            "robustness_pass_threshold": 7.0,
        }
    )

    # Performance settings
    batch_size: int = 10
    max_concurrent_evaluations: int = 3
    cache_results: bool = True

    # Agents (will be initialized later)
    document_evaluation_agent: ClassVar[Optional[object]] = None
    response_evaluation_agent: ClassVar[Optional[object]] = None


class LLMFactory:
    """Factory for creating LLM instances"""

    def __init__(self, config: LLMConfig):
        self.config = config

    def create(self, backend: LLMBackend = LLMBackend.PYDANTIC_AI):
        """Create LLM instance based on backend type"""
        if backend == LLMBackend.PYDANTIC_AI:
            return self._create_pydantic_ai()
        elif backend == LLMBackend.OPENAI:
            return self._create_openai()
        else:
            raise ValueError(f"Unsupported LLM backend: {backend}")

    def _create_pydantic_ai(self):
        """Create Pydantic AI model (placeholder for now)"""
        # This will be implemented when we integrate with pydantic-ai
        model_settings = {
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "timeout": self.config.timeout,
        }
        return None, model_settings

    def _create_openai(self):
        """Create OpenAI client"""
        try:
            import openai

            client = openai.OpenAI(api_key=self.config.api_key, base_url=self.config.api_base)
            return client, {}
        except ImportError:
            raise ImportError("OpenAI package not installed. Run: pip install openai")


@lru_cache()
def get_evaluation_config() -> EvaluationConfig:
    """
    Get evaluation configuration from environment variables or use default values.
    Uses the main Isschat configuration for API keys.
    The function is cached to avoid reloading environment variables on each call.

    Returns:
        EvaluationConfig: Evaluation system configuration
    """
    # Base configuration
    config = EvaluationConfig()

    # Get Isschat configuration for API keys
    try:
        isschat_config = get_isschat_config()
        # Use OpenRouter API key from Isschat config
        config.llm.api_key = isschat_config.openrouter_api_key
    except Exception as e:
        print(f"Warning: Could not load Isschat config: {e}")
        # Fallback to environment variables
        config.llm.api_key = os.getenv("LLM_API_KEY", os.getenv("OPENROUTER_API_KEY", ""))

    # Update LLM configuration from environment variables
    if os.getenv("LLM_MODEL_NAME"):
        config.llm.model_name = os.getenv("LLM_MODEL_NAME")

    # Set OpenRouter base URL if not specified
    if os.getenv("LLM_API_BASE"):
        config.llm.api_base = os.getenv("LLM_API_BASE")

    # LLM parameters
    if os.getenv("LLM_TEMPERATURE"):
        config.llm.temperature = float(os.getenv("LLM_TEMPERATURE"))

    if os.getenv("LLM_MAX_TOKENS"):
        config.llm.max_tokens = int(os.getenv("LLM_MAX_TOKENS"))

    # Database configuration
    if os.getenv("EVALUATION_DB_PATH"):
        config.database.path = os.getenv("EVALUATION_DB_PATH")

    # Mock database for CI tests if specified
    if os.getenv("USE_MOCK_DATABASE", "").lower() in ("true", "1", "yes"):
        config.database.type = DatabaseType.MOCK

    # Reporting configuration
    if os.getenv("EVALUATION_OUTPUT_DIR"):
        config.reporting.output_dir = os.getenv("EVALUATION_OUTPUT_DIR")

    if os.getenv("EVALUATION_DASHBOARD_PORT"):
        config.reporting.dashboard_port = int(os.getenv("EVALUATION_DASHBOARD_PORT"))

    # Performance settings
    if os.getenv("EVALUATION_BATCH_SIZE"):
        config.batch_size = int(os.getenv("EVALUATION_BATCH_SIZE"))

    if os.getenv("EVALUATION_MAX_CONCURRENT"):
        config.max_concurrent_evaluations = int(os.getenv("EVALUATION_MAX_CONCURRENT"))

    # Initialize LLM agents (placeholder for now)
    try:
        model_pydantic_ai, model_pydantic_ai_settings = LLMFactory(config.llm).create(backend=LLMBackend.PYDANTIC_AI)

        # These will be properly initialized when we implement the Agent classes
        # EvaluationConfig.document_evaluation_agent = Agent(...)
        # EvaluationConfig.response_evaluation_agent = Agent(...)

    except Exception as e:
        print(f"Warning: Could not initialize LLM agents: {e}")

    return config


def get_config_debug_info() -> Dict[str, str]:
    """Get debug information about the evaluation configuration"""
    config = get_evaluation_config()

    # Mask sensitive information
    api_key = config.llm.api_key
    api_key_display = f"*****{api_key[-5:]}" if api_key else "Not defined"

    return {
        "llm_model": config.llm.model_name,
        "llm_api_base": config.llm.api_base,
        "llm_api_key": api_key_display,
        "llm_temperature": str(config.llm.temperature),
        "database_type": config.database.type.value,
        "database_path": config.database.path,
        "output_dir": config.reporting.output_dir,
        "dashboard_port": str(config.reporting.dashboard_port),
        "batch_size": str(config.batch_size),
        "max_concurrent": str(config.max_concurrent_evaluations),
    }
