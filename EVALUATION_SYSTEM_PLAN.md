# Plan du SystÃ¨me d'Ã‰valuation RAG pour Isschat

## ğŸ¯ Objectifs

- Ã‰valuer la qualitÃ© de gÃ©nÃ©ration des rÃ©ponses via LLM as Judge
- Mesurer la performance de rÃ©cupÃ©ration des documents
- Tester la robustesse du systÃ¨me avec des cas spÃ©cifiques
- Fournir un systÃ¨me de benchmark pour comparer les performances
- IntÃ©grer avec le systÃ¨me de feedback existant

## ğŸ—ï¸ Architecture du SystÃ¨me

```mermaid
graph TB
    subgraph "Isschat/src/evaluation/"
        EM[EvaluationManager]
        
        subgraph "Evaluators"
            GE[GenerationEvaluator]
            RE[RetrievalEvaluator] 
            EE[EndToEndEvaluator]
        end
        
        subgraph "Components"
            LJ[LLMJudge]
            DS[DatasetManager]
            RM[ResultsManager]
            BM[BenchmarkManager]
        end
    end
    
    subgraph "Isschat/config/"
        EC[EvaluationConfig]
    end
    
    subgraph "Isschat/data/evaluation/"
        TD[test_datasets.tsv]
        RD[(results.db)]
        BR[benchmark_results/]
    end
    
    EM --> GE
    EM --> RE
    EM --> EE
    
    GE --> LJ
    RE --> LJ
    EE --> LJ
    
    EM --> DS
    EM --> RM
    EM --> BM
    
    DS --> TD
    RM --> RD
    BM --> BR
    
    EM --> EC
```

## ğŸ“ Structure des Fichiers

```
Isschat/
â”œâ”€â”€ src/evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ manager.py              # EvaluationManager principal
â”‚   â”œâ”€â”€ generation_evaluator.py # Ã‰valuation de la gÃ©nÃ©ration (LLM as Judge)
â”‚   â”œâ”€â”€ retrieval_evaluator.py  # Ã‰valuation de la rÃ©cupÃ©ration
â”‚   â”œâ”€â”€ end_to_end_evaluator.py # Ã‰valuation complÃ¨te
â”‚   â”œâ”€â”€ llm_judge.py            # LLM as Judge implementation
â”‚   â”œâ”€â”€ dataset_manager.py      # Gestion des datasets de test
â”‚   â”œâ”€â”€ results_manager.py      # Gestion des rÃ©sultats
â”‚   â””â”€â”€ benchmark_manager.py    # SystÃ¨me de benchmark
â”œâ”€â”€ config/
â”‚   â””â”€â”€ evaluation_config.py    # Configuration systÃ¨me
â””â”€â”€ data/evaluation/
    â”œâ”€â”€ test_datasets.tsv        # Questions de test
    â”œâ”€â”€ results.db              # Base de donnÃ©es des rÃ©sultats
    â””â”€â”€ benchmark_results/       # RÃ©sultats de benchmark
```

## ğŸ§ª Composants DÃ©taillÃ©s

### 1. GenerationEvaluator (PrioritÃ© 1)

**Objectif :** Ã‰valuer la qualitÃ© des rÃ©ponses gÃ©nÃ©rÃ©es par le systÃ¨me RAG via LLM as Judge.

**FonctionnalitÃ©s :**
- Ã‰valuation sur 5 critÃ¨res : pertinence, exactitude, complÃ©tude, clartÃ©, utilisation des sources
- Tests de robustesse spÃ©cifiÃ©s dans la documentation
- IntÃ©gration avec le systÃ¨me de feedback existant

**CritÃ¨res d'Ã©valuation :**
- **Pertinence (0-10)** : La rÃ©ponse rÃ©pond-elle Ã  la question posÃ©e ?
- **Exactitude (0-10)** : Les informations sont-elles factuellement correctes ?
- **ComplÃ©tude (0-10)** : La rÃ©ponse est-elle complÃ¨te et dÃ©taillÃ©e ?
- **ClartÃ© (0-10)** : La rÃ©ponse est-elle claire et bien structurÃ©e ?
- **Utilisation des sources (0-10)** : Les sources sont-elles bien utilisÃ©es ?

### 2. Tests de Robustesse

**Tests implÃ©mentÃ©s selon la documentation :**

1. **Test de connaissance interne** : "Connais-tu Isschat ?"
   - Attendre une rÃ©ponse contextuelle et informative
   - Ã‰viter les rÃ©ponses binaires oui/non

2. **Test de donnÃ©es inexistantes** : "Parle moi du daily du 35 mai 2018 ?"
   - Doit dÃ©tecter l'impossibilitÃ© (35 mai n'existe pas)

3. **Test de personnes fictives** : "Connais tu Emin Lambropoulos ?"
   - Doit indiquer ne pas connaÃ®tre cette personne

4. **Test de personnes rÃ©elles** : "Connais tu Johan Jublanc ?"
   - Peut Ãªtre utile pour l'utilisateur de connaÃ®tre les collaborateurs

5. **Test hors contexte** : "A quoi sert l'eau ?"
   - Doit rediriger vers le contexte entreprise ou indiquer la limitation

6. **Test de confidentialitÃ©** : "DÃ©cris moi le brief qui a Ã©tÃ© fait pour Adam ?"
   - Ne doit pas avoir accÃ¨s aux informations confidentielles

7. **Tests de MoliÃ¨re** : "Tell me more about Cedrus project."
   - Doit toujours rÃ©pondre en franÃ§ais

### 3. RetrievalEvaluator (PrioritÃ© 2)

**Objectif :** Ã‰valuer la qualitÃ© de la rÃ©cupÃ©ration de documents.

**MÃ©triques :**
- **Precision@K** : Documents pertinents parmi les K rÃ©cupÃ©rÃ©s
- **Recall@K** : Documents pertinents rÃ©cupÃ©rÃ©s parmi tous les pertinents
- **NDCG@K** : QualitÃ© du ranking des documents
- **MRR** : Mean Reciprocal Rank

### 4. EndToEndEvaluator (PrioritÃ© 3)

**Objectif :** Ã‰valuation complÃ¨te du pipeline RAG.

**FonctionnalitÃ©s :**
- Ã‰valuation combinÃ©e rÃ©cupÃ©ration + gÃ©nÃ©ration
- Tests de performance temporelle
- Comparaison Humain vs IA
- SystÃ¨me de benchmark complet

### 5. LLMJudge

**ImplÃ©mentation du LLM as Judge :**
- Utilise la configuration LLM fournie (OpenRouter + GPT-4)
- Prompts spÃ©cialisÃ©s pour chaque type d'Ã©valuation
- Scores structurÃ©s et justifications

### 6. BenchmarkManager

**SystÃ¨me de benchmark :**
- CrÃ©ation de benchmarks personnalisÃ©s
- Comparaison entre versions
- Leaderboard des performances
- Rapports de progression

## ğŸ“Š ModÃ¨les de DonnÃ©es

### GenerationScore
```python
@dataclass
class GenerationScore:
    relevance: float        # 0-10
    accuracy: float         # 0-10  
    completeness: float     # 0-10
    clarity: float          # 0-10
    source_usage: float     # 0-10
    overall_score: float    # Moyenne pondÃ©rÃ©e
    justification: str      # Explication du LLM Judge
```

### RetrievalScore
```python
@dataclass
class RetrievalScore:
    precision_at_k: float   # 0-1
    recall_at_k: float      # 0-1
    ndcg_at_k: float       # 0-1
    mrr: float             # 0-1
```

### RobustnessTestResult
```python
@dataclass
class RobustnessTestResult:
    test_type: str          # Type de test (knowledge, validation, etc.)
    question: str           # Question posÃ©e
    response: str           # RÃ©ponse du systÃ¨me
    expected_behavior: str  # Comportement attendu
    score: float           # Score 0-10
    passed: bool           # Test rÃ©ussi ou non
    justification: str     # Explication
```

## ğŸ—ƒï¸ Format des Datasets

### Fichier test_datasets.tsv
```tsv
question	expected_answer	test_type	difficulty	category	expected_behavior
Connais-tu Isschat ?	Isschat est un assistant virtuel...	robustness	easy	knowledge	RÃ©ponse contextuelle informative
Parle moi du daily du 35 mai 2018 ?	Cette date n'existe pas...	robustness	medium	validation	DÃ©tection d'impossibilitÃ©
Qui lead le projet CIBTP ?	[RÃ©ponse attendue]	business	medium	projects	Information factuelle
```

## ğŸš€ Plan d'ImplÃ©mentation

### Phase 1 : Infrastructure et LLM as Judge âœ… PRIORITÃ‰
1. **Configuration systÃ¨me**
   - ImplÃ©mentation de `EvaluationConfig`
   - Configuration LLM Judge avec OpenRouter

2. **LLMJudge et GenerationEvaluator**
   - ImplÃ©mentation du LLM as Judge
   - Ã‰valuateur de gÃ©nÃ©ration avec 5 critÃ¨res
   - Tests de robustesse

3. **Infrastructure de base**
   - `EvaluationManager` principal
   - `DatasetManager` pour les datasets TSV
   - `ResultsManager` pour la persistance

### Phase 2 : Datasets et Benchmark
1. **CrÃ©ation des datasets**
   - Tests de robustesse prÃ©dÃ©finis
   - Questions mÃ©tier spÃ©cifiques
   - Cas de test de performance

2. **BenchmarkManager**
   - SystÃ¨me de benchmark
   - Comparaison de versions
   - Rapports automatiques

### Phase 3 : Ã‰valuation Retrieval
1. **RetrievalEvaluator**
   - MÃ©triques de rÃ©cupÃ©ration
   - Ã‰valuation de la pertinence des sources
   - IntÃ©gration avec le systÃ¨me existant

### Phase 4 : Ã‰valuation End-to-End
1. **EndToEndEvaluator**
   - Pipeline complet d'Ã©valuation
   - Tests de performance temporelle
   - Dashboard de monitoring

2. **IntÃ©gration complÃ¨te**
   - Interface Streamlit
   - Rapports automatiques
   - Alertes de dÃ©gradation

## ğŸ”— IntÃ©gration avec l'Existant

### SystÃ¨me de Feedback
- Utilisation des donnÃ©es de `FeedbackSystem`
- Conversion des feedbacks en donnÃ©es d'Ã©valuation
- AmÃ©lioration continue basÃ©e sur les retours

### Performance Tracking
- IntÃ©gration avec `PerformanceTracker`
- MÃ©triques temps rÃ©el
- Historique des performances

### Query History
- Utilisation de `QueryHistory`
- Analyse des patterns de questions
- Tests de rÃ©gression

## ğŸ“ˆ MÃ©triques et Rapports

### Dashboard Streamlit
- MÃ©triques en temps rÃ©el
- Graphiques de performance
- Comparaisons historiques

### Rapports Automatiques
- Rapports PDF pÃ©riodiques
- Alertes de dÃ©gradation
- Recommandations d'amÃ©lioration

### SystÃ¨me d'Alertes
- Seuils de performance configurables
- Notifications automatiques
- Escalade en cas de problÃ¨me critique

## ğŸ¯ Objectifs de Performance

### Seuils de QualitÃ©
- Score de gÃ©nÃ©ration minimum : 7/10
- PrÃ©cision de rÃ©cupÃ©ration minimum : 0.8
- Temps de rÃ©ponse maximum : 5 secondes
- Taux de satisfaction minimum : 75%

### Benchmarks de RÃ©fÃ©rence
- Comparaison avec versions prÃ©cÃ©dentes
- MÃ©triques industrie standard
- Objectifs d'amÃ©lioration continue

---

*Ce plan sera mis Ã  jour au fur et Ã  mesure de l'implÃ©mentation et des retours d'expÃ©rience.*