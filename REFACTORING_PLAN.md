# Plan de Refactoring RAG - Architecture Modulaire et Maintenable

## üéØ **Objectifs du Refactoring**

1. **S√©paration des responsabilit√©s** : D√©coupler les composants pour une meilleure maintenabilit√©
2. **Pipeline offline/online** : S√©parer la pr√©paration des donn√©es de l'utilisation en temps r√©el
3. **Flexibilit√©** : Permettre le changement facile de base de donn√©es vectorielle
4. **Extensibilit√©** : Pr√©parer l'architecture pour les futures strat√©gies de r√©cup√©ration
5. **Code propre** : Structure claire et testable
6. **Int√©gration d'√©valuation** : Interface programmatique pour les syst√®mes d'√©valuation

## üèóÔ∏è **Architecture Propos√©e**

```mermaid
graph TB
    subgraph "OFFLINE PIPELINE (Batch Processing)"
        A[Data Extraction] --> B[Document Filtering]
        B --> C[Chunking]
        C --> D[Post-Processing]
        D --> E[Embedding]
        E --> F[Indexing]
        F --> G[Knowledge Base]
    end
    
    subgraph "ONLINE PIPELINE (Real-time)"
        H[User Query] --> I[Retriever]
        I --> J[Generator]
        J --> K[Response]
        I -.-> G
    end
    
    subgraph "STREAMLIT APP"
        L[Web Interface] --> H
        K --> L
    end
    
    subgraph "EVALUATION SYSTEM"
        M[Dataset] --> N[Evaluator]
        N --> I
        J --> N
        N --> O[Metrics]
    end
    
    subgraph "CONFIGURATION"
        P[Config Manager] -.-> A
        P -.-> I
        P -.-> J
        P -.-> N
    end
```

## üìÅ **Structure de Fichiers Propos√©e**

```
Isschat/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuration centralis√©e
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interfaces.py                # Interfaces abstraites
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py                # Exceptions personnalis√©es
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline/                   # OFFLINE PIPELINE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_extractor.py        # Interface abstraite
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ confluence_extractor.py  # Extraction Confluence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_filter.py       # Filtrage qualit√©
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py              # D√©coupage documents
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ post_processor.py       # Post-traitement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_embedder.py        # Interface abstraite
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ huggingface_embedder.py # Impl√©mentation HF
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline_manager.py         # Orchestrateur pipeline
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ vector_store/                   # INDEXING & STORAGE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_store.py              # Interface abstraite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ faiss_store.py             # Impl√©mentation FAISS
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ store_factory.py           # Factory pattern
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                     # ONLINE RETRIEVAL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_retriever.py          # Interface abstraite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_retriever.py        # R√©cup√©ration simple actuelle
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retriever_factory.py       # Factory pour futures strat√©gies
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ generation/                    # ONLINE GENERATION
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_generator.py          # Interface abstraite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openrouter_generator.py    # G√©n√©rateur OpenRouter
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt_templates.py        # Templates de prompts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag_system/                    # ORCHESTRATION RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py           # Pipeline RAG principal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_formatter.py      # Formatage r√©ponses
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                    # SYST√àME D'√âVALUATION
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py              # √âvaluateur principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics_collector.py       # Collecte de m√©triques
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ batch_processor.py         # Traitement par lots
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ webapp/                        # STREAMLIT INTERFACE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # Application principale
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_page.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin_page.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ history_page.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ sidebar.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ migration/                     # OUTILS DE MIGRATION
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ legacy_adapter.py         # Adaptateur code legacy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_migrator.py      # Migration pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store_migrator.py  # Migration vector store
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migration_dashboard.py    # Suivi migration
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                         # UTILITAIRES
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logging_config.py
‚îÇ       ‚îî‚îÄ‚îÄ helpers.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/                           # SCRIPTS DE GESTION
‚îÇ   ‚îú‚îÄ‚îÄ build_knowledge_base.py        # Script de construction offline
‚îÇ   ‚îú‚îÄ‚îÄ rebuild_index.py              # Script de reconstruction
‚îÇ   ‚îú‚îÄ‚îÄ migrate_to_new_architecture.py # Script de migration
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_system.py            # Script d'√©valuation
‚îÇ   ‚îî‚îÄ‚îÄ health_check.py               # V√©rification syst√®me
‚îÇ
‚îú‚îÄ‚îÄ tests/                            # TESTS
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ migration/
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py
‚îÇ
‚îú‚îÄ‚îÄ config/                           # CONFIGURATIONS
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_config.yaml          # Config pipeline
‚îÇ   ‚îú‚îÄ‚îÄ retrieval_config.yaml         # Config r√©cup√©ration
‚îÇ   ‚îî‚îÄ‚îÄ eval_config.yaml              # Config √©valuation
‚îÇ
‚îú‚îÄ‚îÄ evaluation/                       # DONN√âES D'√âVALUATION
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ benchmarks/
‚îÇ
‚îî‚îÄ‚îÄ data/                            # DONN√âES
    ‚îú‚îÄ‚îÄ raw/                         # Donn√©es brutes
    ‚îú‚îÄ‚îÄ processed/                   # Donn√©es trait√©es
    ‚îî‚îÄ‚îÄ vector_db/                   # Base vectorielle
```

## üîß **Composants D√©taill√©s**

### **1. Interfaces Abstraites (Core)**

```python
# src/core/interfaces.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple

class BaseExtractor(ABC):
    """Interface pour l'extraction de donn√©es"""
    
    @abstractmethod
    def extract(self) -> List[Document]:
        pass

class BaseVectorStore(ABC):
    """Interface pour toutes les bases vectorielles"""
    
    @abstractmethod
    def save_documents(self, documents: List[Dict], embeddings: List[List[float]]) -> None:
        pass
    
    @abstractmethod
    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        pass

class BaseRetriever(ABC):
    """Interface pour tous les retrievers"""
    
    @abstractmethod
    def retrieve(self, query: str, top_k: int = 5) -> Tuple[List[Dict], List[float]]:
        pass

class BaseGenerator(ABC):
    """Interface pour tous les g√©n√©rateurs"""
    
    @abstractmethod
    def generate(self, query: str, context_docs: List[Dict]) -> str:
        pass

class BaseEmbedder(ABC):
    """Interface pour tous les embedders"""
    
    @abstractmethod
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        pass
    
    @abstractmethod
    def embed_query(self, text: str) -> List[float]:
        pass
```

### **2. Data Pipeline (Offline)**

**Extractors** : Extraction modulaire des donn√©es
- Interface `BaseExtractor` pour diff√©rentes sources
- `ConfluenceExtractor` pour l'extraction Confluence actuelle
- Facilite l'ajout de nouvelles sources (SharePoint, fichiers, etc.)

**Processors** : Traitement et pr√©paration
- `DocumentFilter` : Filtrage qualit√© avec scores
- `Chunker` : D√©coupage intelligent avec strat√©gies configurables
- `PostProcessor` : Enrichissement et pr√©paration pour l'indexation

**Embeddings** : Vectorisation modulaire
- Interface `BaseEmbedder` pour diff√©rents mod√®les
- `HuggingFaceEmbedder` pour l'impl√©mentation actuelle
- Facilite le changement de mod√®les d'embedding

### **3. Vector Store (Storage)**

**Abstraction de la base vectorielle** :
- Interface `BaseVectorStore` pour l'ind√©pendance de la DB
- `FAISSStore` pour l'impl√©mentation actuelle
- `StoreFactory` pour instancier la bonne impl√©mentation
- Facilite la migration vers Chroma, Pinecone, etc.

### **4. Retrieval System (Online)**

**R√©cup√©ration modulaire** :
- Interface `BaseRetriever` pour diff√©rentes strat√©gies
- `SimpleRetriever` pour la logique actuelle
- Architecture pr√™te pour Parent/Contextual Retrieval

### **5. Generation System (Online)**

**G√©n√©ration modulaire** :
- Interface `BaseGenerator` pour diff√©rents LLMs
- `OpenRouterGenerator` pour l'impl√©mentation actuelle
- `PromptTemplates` pour la gestion centralis√©e des prompts

### **6. RAG System (Orchestration)**

**Pipeline principal** :
- `RAGPipeline` : Orchestrateur principal retrieval + generation
- `ResponseFormatter` : Formatage et enrichissement des r√©ponses

## üß™ **Syst√®me d'√âvaluation Int√©gr√©**

### **Interface Programmatique Simple**

```python
# Exemple d'utilisation pour l'√©valuation
from src.rag_system.rag_pipeline import RAGPipeline
from src.evaluation.evaluator import IsschatEvaluator

# 1. Configuration pour l'√©valuation
evaluator = IsschatEvaluator(config_path="config/eval_config.yaml")

# 2. √âvaluation sur un dataset
def evaluate_on_dataset(questions_answers_dataset):
    results = []
    
    for item in questions_answers_dataset:
        question = item["question"]
        expected_answer = item["expected_answer"]
        
        # G√©n√©ration de la r√©ponse
        result = evaluator.evaluate_single_query(question)
        
        # Ajout des m√©triques de r√©f√©rence
        result["expected_answer"] = expected_answer
        results.append(result)
    
    return results
```

### **√âvaluateur Principal**

```python
# src/evaluation/evaluator.py
class IsschatEvaluator:
    """√âvaluateur pour Isschat compatible avec n'importe quel framework d'√©val"""
    
    def __init__(self, config_path: str = "config/eval_config.yaml"):
        self.rag_pipeline = RAGPipeline.from_config(config_path)
        self.metrics_collector = MetricsCollector()
    
    def evaluate_single_query(self, question: str, context: dict = None) -> dict:
        """√âvalue une seule question - interface standard pour l'√©val"""
        
        start_time = time.time()
        
        # G√©n√©ration de la r√©ponse
        answer, sources = self.rag_pipeline.process_query(question)
        
        response_time = time.time() - start_time
        
        # Collecte des m√©triques d√©taill√©es
        return {
            "question": question,
            "answer": answer,
            "sources": sources,
            "response_time": response_time,
            "retrieved_documents": self._get_retrieved_docs(),
            "retrieval_scores": self._get_retrieval_scores(),
            "token_count": self._count_tokens(answer),
            "context": context or {}
        }
    
    def evaluate_batch(self, dataset: List[dict]) -> List[dict]:
        """√âvaluation en lot pour de gros datasets"""
        
        results = []
        for item in tqdm(dataset, desc="Evaluating"):
            result = self.evaluate_single_query(
                question=item["question"],
                context=item.get("context", {})
            )
            
            # Ajouter les m√©triques de r√©f√©rence si disponibles
            if "expected_answer" in item:
                result["expected_answer"] = item["expected_answer"]
            
            results.append(result)
        
        return results
```

### **Int√©gration avec Frameworks d'√âvaluation Externes**

```python
# evaluation/integrations/ragas_integration.py
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

def integrate_with_ragas(isschat_evaluator, dataset):
    """Int√©gration avec RAGAS pour l'√©valuation automatique"""
    
    # G√©n√©ration des r√©ponses avec Isschat
    results = isschat_evaluator.evaluate_batch(dataset)
    
    # Conversion au format RAGAS
    ragas_dataset = Dataset.from_dict({
        "question": [r["question"] for r in results],
        "answer": [r["answer"] for r in results],
        "contexts": [[doc["content"] for doc in r["retrieved_documents"]] for r in results],
        "ground_truths": [r.get("expected_answer", "") for r in results]
    })
    
    # √âvaluation avec RAGAS
    ragas_results = evaluate(
        ragas_dataset,
        metrics=[faithfulness, answer_relevancy, context_precision]
    )
    
    return ragas_results

# evaluation/integrations/custom_metrics.py
class CustomMetrics:
    """M√©triques personnalis√©es pour Isschat"""
    
    @staticmethod
    def source_accuracy(retrieved_docs, expected_sources):
        """V√©rifie si les bonnes sources ont √©t√© r√©cup√©r√©es"""
        retrieved_sources = {doc["metadata"]["source"] for doc in retrieved_docs}
        expected_sources = set(expected_sources)
        
        intersection = retrieved_sources.intersection(expected_sources)
        return len(intersection) / len(expected_sources) if expected_sources else 0
    
    @staticmethod
    def response_completeness(answer, expected_keywords):
        """V√©rifie si la r√©ponse contient les mots-cl√©s attendus"""
        answer_lower = answer.lower()
        found_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]
        return len(found_keywords) / len(expected_keywords) if expected_keywords else 0
```

## ‚öôÔ∏è **Configuration YAML**

### **pipeline_config.yaml**
```yaml
data_extraction:
  source_type: "confluence"
  batch_size: 100
  timeout_seconds: 300

document_filtering:
  enabled: true
  min_content_length: 50
  quality_threshold: 0.7

chunking:
  strategy: "recursive"
  chunk_size: 512
  chunk_overlap: 20
  separators: ["\n\n", "\n", ". ", " "]

embedding:
  model_name: "all-MiniLM-L6-v2"
  batch_size: 16
  normalize: true

vector_store:
  type: "faiss"  # Facilite le changement vers "chroma", "pinecone", etc.
  persist_directory: "data/vector_db"
```

### **eval_config.yaml**
```yaml
evaluation:
  mode: "batch"  # Pas d'interface Streamlit
  
retrieval:
  strategy: "simple"
  top_k: 5
  score_threshold: 0.7

generation:
  model: "openai/gpt-4.1-mini"
  temperature: 0.0  # D√©terministe pour l'√©val
  max_tokens: 512

vector_store:
  type: "faiss"
  persist_directory: "data/vector_db"

logging:
  level: "INFO"
  capture_intermediate_results: true  # Pour debugging

metrics:
  custom_metrics:
    - "source_accuracy"
    - "response_completeness"
    - "response_time"
  external_frameworks:
    - "ragas"
```

## üîÑ **Strat√©gie de Migration Progressive**

### **Principe : Migration Sans Interruption de Service**

La migration se fera par **couches successives** en maintenant la compatibilit√© avec l'existant √† chaque √©tape.

### **üìã Phase 0 : Pr√©paration (2-3 jours)**

#### **Analyse de l'Existant**
```mermaid
graph LR
    A[Code Actuel] --> B[Mapping Fonctionnalit√©s]
    B --> C[Points d'Int√©gration]
    C --> D[Plan de Migration]
```

**Actions concr√®tes :**
1. **Audit du code existant** : Identifier toutes les d√©pendances
2. **Cr√©er une branche de migration** : `feature/rag-refactoring`
3. **Tests de r√©gression** : S'assurer que tout fonctionne avant migration
4. **Backup de la base vectorielle** : Sauvegarder `db/` existant

#### **Structure de Transition**
```
Isschat/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ legacy/                    # Code existant (temporaire)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ help_desk.py          # Ancien syst√®me
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_db.py            # Ancien loader
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ isschat_webapp.py     # Ancienne interface
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ new_architecture/          # Nouvelle architecture
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ migration/                 # Adaptateurs de transition
‚îÇ       ‚îú‚îÄ‚îÄ legacy_adapter.py     # Pont ancien/nouveau
‚îÇ       ‚îî‚îÄ‚îÄ compatibility_layer.py
```

### **üîß Phase 1 : Fondations et Coexistence (1 semaine)**

#### **√âtape 1.1 : Cr√©er les Interfaces (Jour 1-2)**

**Objectif** : D√©finir les contrats sans casser l'existant

```python
# src/core/interfaces.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple

class BaseVectorStore(ABC):
    """Interface pour toutes les bases vectorielles"""
    
    @abstractmethod
    def save_documents(self, documents: List[Dict], embeddings: List[List[float]]) -> None:
        pass
    
    @abstractmethod
    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        pass

class BaseRetriever(ABC):
    """Interface pour tous les retrievers"""
    
    @abstractmethod
    def retrieve(self, query: str, top_k: int = 5) -> Tuple[List[Dict], List[float]]:
        pass
```

#### **√âtape 1.2 : Adaptateur Legacy (Jour 2-3)**

**Objectif** : Wrapper l'ancien syst√®me dans les nouvelles interfaces

```python
# src/migration/legacy_adapter.py
from src.legacy.help_desk import HelpDesk
from src.core.interfaces import BaseRetriever

class LegacyHelpDeskAdapter(BaseRetriever):
    """Adaptateur pour utiliser l'ancien HelpDesk avec la nouvelle interface"""
    
    def __init__(self):
        self.legacy_help_desk = HelpDesk(new_db=False)
    
    def retrieve(self, query: str, top_k: int = 5) -> Tuple[List[Dict], List[float]]:
        # Utilise l'ancien syst√®me mais retourne dans le nouveau format
        answer, sources = self.legacy_help_desk.retrieval_qa_inference(query)
        
        # Conversion vers le nouveau format
        docs = self.legacy_help_desk.retriever.invoke(query)
        formatted_docs = [
            {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": 1.0  # Score par d√©faut
            }
            for doc in docs[:top_k]
        ]
        
        return formatted_docs, [1.0] * len(formatted_docs)
```

#### **√âtape 1.3 : Configuration Hybride (Jour 3-4)**

**Objectif** : Syst√®me de configuration qui supporte ancien et nouveau

```python
# src/core/config_manager.py
class HybridConfigManager:
    """Gestionnaire de configuration pour la transition"""
    
    def __init__(self):
        self.use_legacy = True  # Flag de transition
        self.legacy_config = get_config()  # Ancien syst√®me
        self.new_config = None  # Nouveau syst√®me (√† venir)
    
    def get_retriever(self):
        if self.use_legacy:
            return LegacyHelpDeskAdapter()
        else:
            return NewRetrieverImplementation()
    
    def switch_to_new_architecture(self):
        """Bascule vers la nouvelle architecture"""
        self.use_legacy = False
        # Validation que le nouveau syst√®me fonctionne
        self._validate_new_system()
```

### **üèóÔ∏è Phase 2 : Migration du Data Pipeline (1 semaine)**

#### **√âtape 2.1 : Extraction Modulaire (Jour 1-2)**

**Objectif** : Remplacer `load_db.py` par un syst√®me modulaire

```python
# src/data_pipeline/extractors/confluence_extractor.py
class ConfluenceExtractor(BaseExtractor):
    """Nouveau extracteur Confluence bas√© sur l'ancien load_db.py"""
    
    def __init__(self, config: ConfigurationData):
        # R√©utilise la logique de load_db.py mais dans une classe propre
        self.confluence_url = config.confluence_space_name
        self.username = config.confluence_email_address
        # ... reste de la configuration
    
    def extract(self) -> List[Document]:
        # Reprend exactement la logique de load_from_confluence_loader()
        # mais dans une m√©thode propre et testable
        return self._load_from_confluence_loader()
```

#### **√âtape 2.2 : Migration Progressive du Pipeline (Jour 2-4)**

**Strat√©gie** : Cr√©er le nouveau pipeline en parall√®le

```python
# src/migration/pipeline_migrator.py
class PipelineMigrator:
    """G√®re la migration progressive du pipeline de donn√©es"""
    
    def __init__(self):
        self.legacy_loader = DataLoader()  # Ancien syst√®me
        self.new_extractor = ConfluenceExtractor(get_config())
    
    def migrate_data_extraction(self):
        """Migre l'extraction de donn√©es"""
        
        # 1. Extraire avec le nouveau syst√®me
        new_docs = self.new_extractor.extract()
        
        # 2. Comparer avec l'ancien syst√®me
        old_docs = self.legacy_loader.load_from_confluence_loader()
        
        # 3. Validation
        self._validate_extraction_consistency(old_docs, new_docs)
        
        # 4. Si OK, marquer comme migr√©
        self._mark_extraction_migrated()
```

### **üóÑÔ∏è Phase 3 : Migration du Vector Store (1 semaine)**

#### **√âtape 3.1 : Wrapper FAISS (Jour 1-2)**

```python
# src/vector_store/faiss_store.py
class FAISSVectorStore(BaseVectorStore):
    """Wrapper du syst√®me FAISS existant"""
    
    def __init__(self, persist_directory: str):
        self.persist_directory = persist_directory
        self._db = None
    
    def load_existing(self, embeddings):
        """Charge la base FAISS existante"""
        # Utilise exactement la m√™me logique que load_db.py
        self._db = FAISS.load_local(
            self.persist_directory,
            embeddings,
            allow_dangerous_deserialization=True
        )
    
    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        # Utilise la base existante
        return self._db.similarity_search_with_score_by_vector(query_embedding, k=top_k)
```

### **üîç Phase 4 : Migration Retrieval/Generation (1 semaine)**

#### **√âtape 4.1 : Nouveau Retriever (Jour 1-3)**

```python
# src/retrieval/simple_retriever.py
class SimpleRetriever(BaseRetriever):
    """Nouveau retriever bas√© sur la logique existante"""
    
    def __init__(self, vector_store: BaseVectorStore, embedder: BaseEmbedder):
        self.vector_store = vector_store
        self.embedder = embedder
    
    def retrieve(self, query: str, top_k: int = 5) -> Tuple[List[Dict], List[float]]:
        # Reprend la logique de HelpDesk.retrieval_qa_inference
        # mais dans une architecture propre
        
        # 1. Embedding de la requ√™te
        query_embedding = self.embedder.embed_query(query)
        
        # 2. Recherche dans le vector store
        results = self.vector_store.search(query_embedding, top_k)
        
        # 3. Formatage des r√©sultats
        return self._format_results(results)
```

### **üñ•Ô∏è Phase 5 : Migration Interface Streamlit (1 semaine)**

#### **√âtape 5.1 : Nouveau RAG Pipeline (Jour 1-2)**

```python
# src/rag_system/rag_pipeline.py
class RAGPipeline:
    """Pipeline RAG principal qui remplace HelpDesk"""
    
    def __init__(self, retriever: BaseRetriever, generator: BaseGenerator):
        self.retriever = retriever
        self.generator = generator
        self.last_retrieved_docs = []
        self.last_response_time = 0
    
    def process_query(self, query: str) -> Tuple[str, str]:
        """Traite une requ√™te - interface compatible avec l'ancien syst√®me"""
        
        start_time = time.time()
        
        # 1. R√©cup√©ration
        docs, scores = self.retriever.retrieve(query)
        self.last_retrieved_docs = docs
        
        # 2. G√©n√©ration
        answer = self.generator.generate(query, docs)
        
        # 3. Sources (format compatible)
        sources = self._format_sources(docs)
        
        self.last_response_time = time.time() - start_time
        
        return answer, sources
    
    @classmethod
    def from_config(cls, config_path: str):
        """Factory method pour cr√©er le pipeline depuis la config"""
        config = load_config(config_path)
        
        # Initialisation des composants
        vector_store = StoreFactory.create_store(config.vector_store)
        embedder = EmbedderFactory.create_embedder(config.embedding)
        retriever = RetrieverFactory.create_retriever(config.retrieval, vector_store, embedder)
        generator = GeneratorFactory.create_generator(config.generation)
        
        return cls(retriever, generator)
```

### **üîÑ Phase 6 : Bascule et Nettoyage (1 semaine)**

#### **√âtape 6.1 : Tests de Charge (Jour 1-2)**

```python
# tests/migration/test_performance.py
def test_performance_comparison():
    """Compare les performances ancien vs nouveau syst√®me"""
    
    queries = ["test query 1", "test query 2", ...]
    
    # Test ancien syst√®me
    old_times = []
    for query in queries:
        start = time.time()
        old_help_desk.retrieval_qa_inference(query)
        old_times.append(time.time() - start)
    
    # Test nouveau syst√®me
    new_times = []
    for query in queries:
        start = time.time()
        new_rag_pipeline.process_query(query)
        new_times.append(time.time() - start)
    
    # Validation que les performances sont similaires ou meilleures
    assert np.mean(new_times) <= np.mean(old_times) * 1.1  # Max 10% plus lent
```

#### **√âtape 6.2 : Bascule D√©finitive (Jour 3-4)**

```python
# scripts/migrate_to_new_architecture.py
def perform_final_migration():
    """Script de bascule d√©finitive"""
    
    print("üîÑ D√©but de la migration finale...")
    
    # 1. Validation compl√®te
    validator = MigrationValidator()
    if not validator.validate_all_systems():
        raise Exception("Validation √©chou√©e - migration annul√©e")
    
    # 2. Backup de s√©curit√©
    backup_manager = BackupManager()
    backup_manager.create_full_backup()
    
    # 3. Bascule des flags
    config_manager = HybridConfigManager()
    config_manager.switch_to_new_architecture()
    
    # 4. Tests post-migration
    post_migration_tests()
    
    print("‚úÖ Migration termin√©e avec succ√®s!")
```

## üöÄ **Plan d'Impl√©mentation**

### **Phase 1 : Fondations (Semaine 1)**
1. Cr√©er la structure de dossiers
2. Impl√©menter les interfaces abstraites
3. Migrer la configuration vers le nouveau syst√®me
4. Cr√©er les exceptions personnalis√©es

### **Phase 2 : Data Pipeline (Semaine 2)**
1. Refactoriser l'extraction Confluence
2. Impl√©menter le syst√®me de filtrage
3. Modulariser le chunking
4. Cr√©er le pipeline manager

### **Phase 3 : Vector Store (Semaine 3)**
1. Abstraire FAISS dans l'interface
2. Impl√©menter le factory pattern
3. Tester la flexibilit√© avec une autre DB (Chroma)

### **Phase 4 : Retrieval & Generation (Semaine 4)**
1. Refactoriser