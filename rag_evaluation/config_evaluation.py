"""
Centralized configuration for the RAG evaluation system.

This module defines the shared configuration parameters between the RAG system
and the evaluation system.
"""

import os
import sys
from dataclasses import dataclass
from functools import lru_cache
from pydantic import BaseModel
from pydantic_ai import Agent
from dotenv import load_dotenv
from typing import Dict, Any, Optional, ClassVar

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from config import LLMConfig, LLMPrompt, LLMFactory, LLMBackend
from models import DocumentEvaluation, ResponseEvaluation

load_dotenv()


@dataclass
class DatabaseType:
    """Type of database to use."""

    REAL = "real"  # Real database (FAISS, etc.)
    MOCK = "mock"  # Mock database for tests


class DatabaseConfig(BaseModel):
    """Database configuration."""

    type: DatabaseType = DatabaseType.REAL
    persist_directory: str = "db"
    mock_data: Optional[Dict[str, Any]] = None


class PromptConfig(BaseModel):
    """Configuration for prompts."""

    rag_template: str = LLMPrompt.PROMPT_TEMPLATE

    document_evaluation_template: str = """
    You are an expert judge tasked with evaluating the relevance of documents retrieved by a RAG system.

    Question asked to the system: {question}

    Context that should have been retrieved:
    {expected_context}

    Documents actually retrieved:
    {retrieved_documents}

    Assess whether the retrieved documents are relevant to the question and the expected context.
    Respond only with "rather good" if the documents contain the sought information,
    or "rather bad" if they are not relevant.

    Provide a brief reason for your assessment.
    """

    response_evaluation_template: str = """
    You are an expert judge tasked with evaluating the quality of responses from a Confluence virtual assistant.

    Question asked: {question}

    Expected answer:
    {expected_answer}

    Response generated by the system:
    {generated_response}

    Evaluate if the generated response correctly answers the question and contains the key information
    present in the expected answer. The response should be in French and adopt a professional tone.

    Respond only with "rather good" if the answer is generally correct and relevant,
    or "rather bad" if it is incorrect or incomplete.

    Provide a brief reason for your assessment.
    """


class EvaluationConfig(BaseModel):
    """General configuration for the evaluation system."""

    llm: LLMConfig = LLMConfig()
    database: DatabaseConfig = DatabaseConfig()
    prompts: PromptConfig = PromptConfig()
    evaluation_dataset_path: str = "data/evaluation_dataset.tsv"
    result_output_path: str = "data/evaluation_results.json"
    report_output_path: str = "data/evaluation_report.md"
    document_evaluation_agent: ClassVar[Agent] = None
    response_evaluation_agent: ClassVar[Agent] = None


@lru_cache()
def get_config() -> EvaluationConfig:
    """
    Get configuration from environment variables or use default values.

    The function is cached to avoid reloading environment variables on each call.

    Returns:
        EvaluationConfig: Evaluation system configuration
    """
    # Base configuration
    config = EvaluationConfig()

    # Update with environment variables or use OpenRouter key if LLM_API_KEY is not defined
    if os.getenv("LLM_MODEL_NAME"):
        config.llm.model_name = os.getenv("LLM_MODEL_NAME")

    # First try to use LLM_API_KEY, if not available use OPENROUTER_API_KEY
    config.llm.api_key = os.getenv("LLM_API_KEY", os.getenv("OPENROUTER_API_KEY", LLMConfig().api_key))

    # Set OpenRouter base URL if not specified
    config.llm.api_base = os.getenv("LLM_API_BASE", LLMConfig().api_base)

    # Mock database for CI tests if specified
    if os.getenv("USE_MOCK_DATABASE", "").lower() in ("true", "1", "yes"):
        config.database.type = DatabaseType.MOCK

    model_pydantic_ai, model_pydantic_ai_settings = LLMFactory(config.llm).create(backend=LLMBackend.PYDANTIC_AI)

    EvaluationConfig.document_evaluation_agent = Agent(
        description="An expert judge evaluating the relevance of documents retrieved by a RAG system",
        output_type=DocumentEvaluation,
        model=model_pydantic_ai,
        model_settings=model_pydantic_ai_settings,
    )

    EvaluationConfig.response_evaluation_agent = Agent(
        description="An expert judge evaluating the quality of responses generated by a virtual assistant",
        output_type=ResponseEvaluation,
        model=model_pydantic_ai,
        model_settings=model_pydantic_ai_settings,
    )

    return config
