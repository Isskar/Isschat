"""
LLM evaluator for the RAG evaluation system.

This module contains the logic to evaluate the relevance of retrieved documents
and the quality of generated responses using an LLM as judge.
"""

from typing import List, Dict, Any

# Use explicit relative imports
from config_evaluation import EvaluationConfig, PromptConfig
from models import DocumentEvaluation, ResponseEvaluation
from factory import ComponentFactory


class LLMEvaluator:
    """Evaluator using the configured LLM to assess documents and responses."""

    def __init__(self, config: EvaluationConfig):
        """
        Initialize the LLM evaluator.

        Args:
            config: Evaluation system configuration
        """
        self.config = config
        self.llm = ComponentFactory.create_llm(config)

    def evaluate_documents(
        self, question: str, expected_context: str, retrieved_documents: List[Dict[str, Any]]
    ) -> DocumentEvaluation:
        """
        Evaluate the relevance of retrieved documents compared to expected context.

        Args:
            question: Question asked to the system
            expected_context: Description of the context that should have been retrieved
            retrieved_documents: List of documents retrieved by the system

        Returns:
            DocumentEvaluation: Binary evaluation with explanation
        """
        # Format documents for display
        docs_str = "\n".join(
            [
                f"Document {i + 1}:\nTitle: {doc.get('title', 'Unknown')}\nContent: {doc.get('content', 'Empty')[:300]}..."
                for i, doc in enumerate(retrieved_documents)
            ]
        )

        prompt = PromptConfig().document_evaluation_template.format(
            question=question,
            expected_context=expected_context,
            retrieved_documents=docs_str,
        )

        result = self.config.document_evaluation_agent.run_sync(user_prompt=prompt)
        print(f"DEBUG - Result type: {type(result.output)}")
        return result.output

    def evaluate_response(self, question: str, expected_answer: str, generated_response: str) -> ResponseEvaluation:
        """
        Evaluate the quality of the generated response compared to the expected answer.

        Args:
            question: Question asked to the system
            expected_answer: Ideal response expected
            generated_response: Response generated by the system

        Returns:
            ResponseEvaluation: Binary evaluation with explanation
        """

        prompt = PromptConfig().response_evaluation_template.format(
            question=question,
            expected_answer=expected_answer,
            generated_response=generated_response,
        )

        result = self.config.response_evaluation_agent.run_sync(user_prompt=prompt)
        print(f"DEBUG - Result type: {type(result.output)}")
        return result.output
